<!DOCTYPE html>
<html>
<head>
<title>LLMagogy or fast multi-stage pre-training of Large Language Models with pre-selected limited datasets. Version 2.</title>
<style>
body {
max-width: 800px;
margin: 0 auto;
padding: 20px;
font-size: 18px;
line-height: 1.5;
}
p {
text-align: justify;
}
</style>
</head>
<body>
<h1>LLMagogy or fast multi-stage pre-training of Large Language Models with pre-selected limited datasets. Version 2.</h1>
<p><i>“knowing a few principles frees you from knowing many facts” R. Descartes<br>
“the knowledge of certain principles easily compensates the lack of knowledge of certain facts” C. Helvétius</i></p>
<h2>Abstract</h2>
<p>This paper experimentally tests the hypothesis regarding the benefits of multi-stage pretraining of large language models (LLMs), an approach inspired by human learning. Instead of traditionally training a fixed-size model on a huge, chaotic corpus, we propose gradually "growing" the model: starting with a small architecture and expanding its depth and breadth in successive stages, all the while training on increasingly comprehensive datasets.
Experiments on a controlled corpus of Python documentation show that the key idea—dividing the training material into stages of increasing complexity—is not supported: this approach is inferior to training on the full dataset at all stages. However, the strategy of gradually growing the model on a single dataset proves effective. It allows for a reduction in overall training time by 5–25% without loss of quality, and sometimes even improves model accuracy.
The mechanisms for controlling learning during expansion are also explored: warming up new weights demonstrates a moderate improvement in stability, while partially freezing old parameters yields no positive effect. The results indicate that the advantage of the stepwise approach is not due to the "pedagogical" structure of the data, but to the computational efficiency of training small models in the early stages. The findings suggest directions for future research, including the use of datasets with repetitive information and optimizing the number of growth stages.</p>

<p>This repository contains the implementation and experimental results of **LLMagogyV2**, a research project exploring **progressive model growth** as an alternative to traditional end-to-end pre-training of language models.
This is a continuation of the first version of the <a href="https://github.com/loftyara/LLMagogy"><LLMagogy/a></p>

<p>Unlike standard approaches that train a fixed-size model on massive chaotic corpora, LLMagogy trains a **small model first** and **gradually expands it** (depth and width) while training on a **structured, cumulative dataset**—inspired by human pedagogy.</p>

<p>The core idea was **not confirmed**: staged training on *non-repetitive, cumulative subsets* does **not** outperform full-dataset training.  
However, **progressive growth on the full dataset** consistently **reduces training time by 5–25%** without quality loss or even with best quality.</p>

<p>All code, datasets, and results are provided for full reproducibility.</p>

<a href="https://github.com/loftyara/LLMagogyV2">continue on github</a>
</body>
</html>