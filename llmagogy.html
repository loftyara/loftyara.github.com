<!DOCTYPE html>
<html>
<body>
<h1>LLMagogy or fast multi-stage pre-training of Large Language Models with pre-selected limited datasets</h1>
<p><i>“knowing a few principles frees you from knowing many facts” R. Descartes<br>
“the knowledge of certain principles easily compensates the lack of knowledge of certain facts” C. Helvétius</i></p>
<p>This article proposes a novel multi-stage pre-training approach for large language models (LLMs), inspired by human learning processes. Traditional LLM pre-training involves training massive models on trillions of tokens, requiring significant computational resources and time. In contrast, the proposed method mimics human education by gradually "maturing" the model through stages, starting with small, curated datasets (e.g., textbooks) and progressively increasing model complexity and data volume. Key innovations include:<br>
<ul>
<li>dividing training into stages corresponding to human developmental phases,</li>
<li>simulating model "growth" by incrementally increasing network depth (D) and width (W)</li>
<li>preserving previously learned knowledge using a freezing coefficient (F) to stabilize earlier parameters.</li>
</ul>
This approach aims to create models that understand, rather than memorize, information, while reducing resource consumption. The method remains theoretical and requires experimental validation, but it offers a promising alternative to conventional LLM training, with potential benefits in efficiency, control, and model interpretability.</p>
<a href="https://github.com/loftyara/LLMagogy">continue on github</a>
</body>
</html>